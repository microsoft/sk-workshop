{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. Licensed under the MIT license.\n",
    "\n",
    "### LAB02 - Introduction to Semantic Kernel\n",
    "\n",
    "In this lab, we will learn how to use Semantic Kernel to interact with Large Language Models. We'll complete the following steps:\n",
    "\n",
    "1) Import general dependencies.\n",
    "2) Using Semantic Kernel SDK to run prompts on a Text Completion model from Azure OpenAI. \n",
    "3) Instantiate a Kernel to add LLM capabilities to your application.\n",
    "4) Connect the Kernel to a Completion model to run your first semantic function.\n",
    "5) Run Chat Completion prompts using Semantic Kernel.\n",
    "\n",
    "**Important:** We're a using a method to automatically read sensitive parameters from a `.env` file. Therefore, please ensure that you have a `.env` file in the same directory of this notebook with the following parameters:\n",
    "\n",
    "``` python\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=\"NAME OF YOUR MODEL DEPLOYMENT\"\n",
    "AZURE_OPENAI_ENDPOINT=\"https://<YOUR_ENDPOINT>.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY=\"KEY\"\n",
    "```\n",
    "Optionally, you can update the values and run the cell below to create your `.env` file. Please note the deployment name as we'll use both **Text Completion** and **Chat Completion** in this LAB. Initially, we'll use a TEXT COMPLETION model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>Copyright (c) Microsoft Corporation.\n",
    "Licensed under the MIT license.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=\"\"\n",
    "AZURE_OPENAI_ENDPOINT=\"\"\n",
    "AZURE_OPENAI_API_KEY=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Import Dependencies\n",
    "\n",
    "**Semantic Kernel** is a SDK currently available in C#, Java and Python. However, each language support different at the current stage. Please visit [supported languages](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages) to check which features are supported per language.\n",
    "\n",
    "In this lab, we'll use Python SDK which can be obtained in Pypi repository.\n",
    "\n",
    "**Important:** if you get an error running the code below, make sure that you've completed the setup process. Optionally, you can run `pip install semantic_kernel` in your terminal or `!python -m pip install semantic_kernel` in a Python code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Semantic Kernel SDK that will be used in this lab. \n",
    "#The other dependencies will be imported directly in the step where they are used.\n",
    "import semantic_kernel as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Using Semantic Kernel SDK to run prompts on a Text Completion model from Azure OpenAI\n",
    "\n",
    "In this step, we'll connect to an **Azure OpenAI Service** and use a Text Completion model. \n",
    "The Semantic Kernel also supports **OpenAI**. Other AI Services, like **Hugging Face** will be supported shortly. You can visit this [link](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages#ai-service-endpoints) to check the most updated list.\n",
    "\n",
    "In this example, we're using **AzureTextCompletion** module to interact with Text Completion models from Azure OpenAI, such as `gpt-35-turbo-instruct` or `text-davinci-003`. This is suitable for text generation activities. We are also using a module to control the behavior of the requests, defining parameters like Temperature, max_tokens and even how many responses we want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Azure Text Completion service connector\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion\n",
    "#Importing the module that contains the settings for the Azure Text Completion service\n",
    "from semantic_kernel.connectors.ai import CompleteRequestSettings\n",
    "\n",
    "# Read the model, API key and endpoint from the .env file\n",
    "deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "\n",
    "# Create the Azure Text Completion service connector\n",
    "azure_text_completion = AzureTextCompletion(deployment_name=deployment, endpoint=endpoint, api_key=api_key)\n",
    "\n",
    "# Create the settings to control the behavior to the requests of Azure Text Completion service\n",
    "request_settings = CompleteRequestSettings(\n",
    "    max_tokens=20,\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    number_of_responses=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all set, let's run a simple prompt to generate taglines using the Text Completion model. Please note that the number of results, response lenght and temperature are controlled by the **CompleteRequestSettings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: \n",
      "\n",
      "\"Connecting you to the world, one call at a time.\"\n",
      "Result 2: \n",
      "\n",
      "\"Connecting you to the world, one call at a time.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write tagline for a telco company.\"\n",
    "\n",
    "#Calling the Azure Text Completion service using Semantic Kernel SDK and AI Service Connector\n",
    "results = await azure_text_completion.complete_async(prompt, request_settings)\n",
    "\n",
    "#Printing out the results\n",
    "i = 1\n",
    "for result in results:\n",
    "    print(f\"Result {i}: {result}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to use the SDK to run a prompt, let's start building out our Call Center Analytics Solution. We have already explored how to use prompts to summarize and extract relevant information from call center conversations. As our next step, let's run them in Semantic Kernel. We'll explore different methods.\n",
    "\n",
    "We'll use the conversation below as an example. If you prefer, just change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a call center conversation between a customer and an agent. You can replace by your own example\n",
    "input = \"\"\"\n",
    "Agent: Hello, welcome to Telco's customer service. My name is Juan, how can I assist you?\n",
    "Client: Hello, Juan. I'm calling because I'm having issues with my mobile data plan. It's very slow and I can't browse the internet or use my apps.\n",
    "Agent: I'm very sorry for the inconvenience, sir. Could you please tell me your phone number and your full name?\n",
    "Client: Yes, sure. My number is 011-4567-8910 and my name is Martín Pérez.\n",
    "Agent: Thank you, Mr. Pérez. I'm going to check your plan and your data usage. One moment, please.\n",
    "Client: Okay, thank you.\n",
    "Agent: Mr. Pérez, I've reviewed your plan and I see that you have the basic plan of 2 GB of data per month. Is that correct?\n",
    "Client: Yes, that's right.\n",
    "Agent: Well, I inform you that you have consumed 90% of your data limit and you only have 200 MB available until the end of the month. That's why your browsing speed has been reduced.\n",
    "Client: What? How is that possible? I barely use the internet on my cell phone. I just check my email and my social networks from time to time. I don't watch videos or download large files.\n",
    "Agent: I understand, Mr. Pérez. But keep in mind that some applications consume data in the background, without you realizing it. For example, automatic updates, backups, GPS, etc.\n",
    "Client: Well, but they didn't explain that to me when I contracted the plan. They told me that with 2 GB I would have enough for the whole month. I feel cheated.\n",
    "Agent: I apologize, Mr. Pérez. It was not our intention to deceive you. I offer you a solution: if you want, you can change your plan to a higher one, with more GB of data and higher speed. This way you can enjoy a better browsing experience.\n",
    "Client: And how much would that cost me?\n",
    "Agent: We have a special offer for you. For only 10 pesos more per month, you can access the premium plan of 5 GB of data and 4G speed. Are you interested?\n",
    "Client: Mmm, I don't know. Isn't there another option? Can't you give me more speed without charging me more?\n",
    "Agent: I'm sorry, Mr. Pérez. That's the only option we have available. If you don't change your plan, you'll have to wait until next month to recover your normal speed. Or you can buy an additional data package, but it would be more expensive than changing plans.\n",
    "Client: Well, let me think about it. Can I call later to confirm?\n",
    "Agent: Of course, Mr. Pérez. You can call whenever you want. The number is the same one you dialed now. Is there anything else I can help you with?\n",
    "Client: No, that's all. Thank you for your attention.\n",
    "Agent: Thank you, Mr. Pérez. Have a good day. Goodbye.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the same prompt for \"Problem Summarization\". Note that we're using python strings to append the Conversation to our prompt. As you run this cell, we should see a summary similar to what we saw in Azure AI Studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "Customer has slow internet and data issues, requests help from Agente. Agente verifies customer's plan and data usage, explains that they have reached their limit and suggests upgrading for a better experience. Customer feels misled and asks for other options, but is offered a special upgrade deal or the option to wait until next month or purchase an additional data package. Customer wants time to consider and can call back later. Outcome is unresolved, customer thanked Agente for assistance.\n"
     ]
    }
   ],
   "source": [
    "#Initially, let's use a prompt similar to what we have tested in the AI Studio.\n",
    "prompt = f\"\"\"\n",
    "Summarize the problem described below. \n",
    "Be brief, yet informative.\n",
    "Try to capture in the summary:\n",
    "- The problem description;\n",
    "- The product customer is complaining (if available);\n",
    "- How Agente, handled the call;\n",
    "- The outcome of the call. \n",
    "\n",
    "[INPUT]\n",
    "{input}\n",
    "[END INPUT]\n",
    "\"\"\"\n",
    "\n",
    "# Create the settings to control the behavior to the requests of Azure Text Completion service\n",
    "request_settings = CompleteRequestSettings(\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    number_of_responses=1\n",
    ")\n",
    "\n",
    "#Calling the Azure Text Completion service using Semantic Kernel SDK and AI Service Connector\n",
    "result = await azure_text_completion.complete_async(prompt, request_settings)\n",
    "\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Instantiate a Kernel to add LLM capabilities to your application\n",
    "The core of Semantic Kernel is it's Kernel. It's responsible to transform an User Input (or ASK) into a response, optionally orchestrating Plugins in this process. It's also what makes Semantic Kernel shines as it can transform existing applications in AI applications.\n",
    "\n",
    "In this step, we'll instantiate a simple Kernel with no additonal options. Optionally, we could define Logging, Memory and the set of Plugins we want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the kernel\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "\n",
    "#Example of how to instantiate the kernel with a different log level\n",
    "#logger = sk.NullLogger()\n",
    "#kernel_with_logging = sk.Kernel(log=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Connect the Kernel to a Completion model to run your first semantic function.\n",
    "Now that we have our kernel, we can start adding capabilities to it. We already saw how to use the SDK and connectors to the AI services to run prompts. Now, let's see how can we use the Kernel to transform a prompt into a semantic function able to run Text Completion activities using a LLM.\n",
    "\n",
    "The first step is to add to our kernel the Text Completion model that we already have defined before. We can add multiple models and give then names (or nicknames) to later decide which ones we want to use and define programmatically which one is the default.\n",
    "\n",
    "You can also add Chat Completion models, like GPT-4 and GPT-3.5-Turbo, as per the commented code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<semantic_kernel.kernel.Kernel at 0x7fc6a1c990c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the text completion service to the kernel\n",
    "kernel.add_text_completion_service(\"Text Completion\", \n",
    "                                   AzureTextCompletion(deployment_name=deployment, endpoint=endpoint, api_key=api_key))\n",
    "\n",
    "#We can add several models of the same type to the kernel\n",
    "#kernel.add_text_completion_service(\"Other Text Completion\",\n",
    "#                                   AzureTextCompletion(\"text-curie-001\", endpoint, api_key))\n",
    "\n",
    "#You can also define a default Text Completion / Chat Completion service to be used by the kernel \n",
    "#kernel.set_default_text_completion_service(\"Text Completion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the kernel connected to a large language model from Azure OpenAI Service, we can start using it to run Text Completion operations. The best way to make our prompts reusable is to create a **Semantic Function**. You can create and add one to the Kernel based on a **Prompt**. This is simple a textual description, or instruction, of what we want the function to do. Optionally, we can add parameters, like `{{$input}}`, to inject values in runtime.\n",
    "\n",
    "We'll deep dive in Semantic Functions later and now we'll demonstrate a simplified way to create and run Semantic Function **inline**. With this method we can start structuring our application and transforming Prompts into functions. Please note that as we define the function we can also control the behavior of how we expect the kernel to run it, for example, controlling the max number of tokens in the response and the temperature. This we'll ensure that the function we'll run consistenly.\n",
    "\n",
    "Therefore, in this step, we'll create an **inline semantic function** to classify a telco problem in different categories, like Mobile Internet, Fixed Internet, etc. This is based on the same example we run in Azure AI Studio and is one of the things we need to complete in our Call Center Analytics Solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the prompt for categorizing based on the problem description\n",
    "prompt = \"\"\"\n",
    "Problem Description: {{$input}}\n",
    "Classify the problem description above as one of these categories: Fixed Internet, Mobile Internet, TV, Landline, Billing.\n",
    "Only write the output category with no extra text.\n",
    "Only write one category per problem description.\n",
    "\"\"\"\n",
    "\n",
    "# Create a semantic function for classifying the problem description\n",
    "classify_call_function = kernel.create_semantic_function(prompt, \n",
    "                                                         max_tokens=10,\n",
    "                                                         temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our Kernel has a new function to Categorize problems, we'll test it based on a problem description. We expected as a result one of the classifications previously defined in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: \n",
      "Mobile Internet\n"
     ]
    }
   ],
   "source": [
    "#Invoke the semantic function as a regular Python function\n",
    "try:\n",
    "    classification = classify_call_function(input)\n",
    "    print(f\"Classification: {classification}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also register the semantic function and already define important information to later reuse the function. The code below is an example and you can also use it to define semantic functions inline. This notation will be useful in the future as we create our AI Plugins, which we'll explore later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import PromptTemplate, PromptTemplateConfig, SemanticFunctionConfig\n",
    "\n",
    "#Define a configuration to control how the prompt will run in the model and define descriptions that can later be used by planners\n",
    "prompt_config = PromptTemplateConfig(\n",
    "    description=\"Classify a problem report in a call to the telco company based on provided categories.\",\n",
    "    type=\"completion\",\n",
    "    completion=PromptTemplateConfig.CompletionConfig(0.5, 0.0, 0.0, 0.0, 10),\n",
    "    input=PromptTemplateConfig.InputConfig(\n",
    "        parameters=[\n",
    "            PromptTemplateConfig.InputParameter(\n",
    "                name=\"input\", description=\"The problem reported by the user that needs to be classified. Required parameter.\", default_value=\"\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Transform the Prompt into a PromptTemplate object\n",
    "prompt_template = PromptTemplate(\n",
    "    template=prompt,\n",
    "    template_engine=kernel.prompt_template_engine,\n",
    "    prompt_config=prompt_config\n",
    ")\n",
    "\n",
    "#Defines the semantic function configuration set\n",
    "categorize_fn_config = SemanticFunctionConfig(prompt_config, prompt_template)\n",
    "\n",
    "#Finally, register the new semantic function as part of an AI plugin. Later we'll explore this notation in more details\n",
    "categorize_fn = kernel.register_semantic_function(\"CallCenterPlugin\",\"Categorize\", categorize_fn_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "Mobile Internet\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Call the classify function with a problem description\n",
    "    result = await kernel.run_async(categorize_fn, input_str=input)\n",
    "    # Print the result\n",
    "    print(f\"Result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 - Run Chat Completion prompts using Semantic Kernel\n",
    "\n",
    "Models like **GPT-4** and **GPT-35-Turbo** were trained to accept inputs formatted as a **Conversation**. They use a different type of API called **Chat Completion**. Therefore, we need to make some adaptations to run prompts in these models as they require to use a different notation with messages considering three roles:\n",
    "- **System:** used to to give instructions/context about to the model and how it should behave. Also known as System Message or Metaprompt.\n",
    "- **User:** used to represent messages from the user. This is the messages we'll try to respond. Also refered as User Message.\n",
    "- **Assistant:** used to represent messages/responses from the model. Also refered as Assistant Message.\n",
    "\n",
    "Now, let's see how we can run Chat Completions in Semantic Kernel.\n",
    "\n",
    "**Important:** As we'll now run **Chat Completion** operations, you need to review and run the cell below to recreate a `.env` file with a **Chat Completion** model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=\"\"\n",
    "AZURE_OPENAI_ENDPOINT=\"\"\n",
    "AZURE_OPENAI_API_KEY=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we need to import the correct connector for AzureChatCompletion and add a Chat Completion model to the kernel. After that, it's important to correctly setup our prompt adding the different roles (System, Assistant, User). Eventually, you can use this notation to add more messages to act as few-shot examples to obtain better results from the model.\n",
    "\n",
    "Please note that we're using as **System Message** the example we ran in Azure AI Studio for our Call Center Analytics solution to extract additional information from a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Semantic Kernel SDK that will be used in this lab. \n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "#Instantiating the kernel\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "# Read the model, API key and endpoint from the .env file\n",
    "deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "\n",
    "kernel.add_chat_service(\"Chat Completion\", \n",
    "                        AzureChatCompletion(deployment_name=deployment, endpoint=endpoint, api_key=api_key))\n",
    "\n",
    "#Define a system message to model how the model should behave\n",
    "system_message = \"\"\"\n",
    "You're an AI assistant that helps telco company to extract valuable information from their conversations by creating JSON files for each conversation transcription you receive. You always try to extract and format as a JSON:\n",
    "1. Customer Name [name]\n",
    "2. Customer Contact Phone [phone]\n",
    "3. Main Topic of the Conversation [topic]\n",
    "4. Customer Sentiment (Neutral, Positive, Negative)[sentiment]\n",
    "5. How the Agent Handled the Conversation [agent_behavior]\n",
    "6. What was the FINAL Outcome of the Conversation [outcome]\n",
    "7. A really brief Summary of the Conversation [summary]\n",
    "\n",
    "Only extract information that you're sure. If you're unsure, write \"Unknown/Not Found\" in the JSON file.\n",
    "\"\"\"\n",
    "\n",
    "#Defining the chat Completion parameters, like max_tokens, temperature, top_p, etc.\n",
    "prompt_config = sk.PromptTemplateConfig.from_completion_parameters(\n",
    "    max_tokens=500, temperature=0.7, top_p=0.5\n",
    ")\n",
    "\n",
    "#Defining a prompt template setup the Three Roles in a Chat Completion Operation. Here we're defining input for the User Message\n",
    "prompt_template = sk.ChatPromptTemplate(\n",
    "    \"{{$input}}\", kernel.prompt_template_engine, prompt_config\n",
    ")\n",
    "\n",
    "#Ensure our prompt template have at least a System Message\n",
    "prompt_template.add_system_message(system_message)\n",
    "#Optionally, we can load user and assistant messages to use as Few Shot examples\n",
    "prompt_template.add_user_message(\"Hello, how are you?\")\n",
    "prompt_template.add_assistant_message(\"Hello! I can help you to extract valuable information from Call Center conversations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our prompt ready and configured, we'll register it as a new inline Semantic Function and invoke it as a function. We should see important information extracted from our conversation in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction: {\n",
      "  \"name\": \"Martín Pérez\",\n",
      "  \"phone\": \"011-4567-8910\",\n",
      "  \"topic\": \"Problemas con el plan de datos móviles\",\n",
      "  \"sentiment\": \"Negative\",\n",
      "  \"agent_behavior\": \"El agente fue paciente, explicativo y ofreció una solución al problema del cliente\",\n",
      "  \"outcome\": \"El cliente considerará la opción de cambiar su plan y llamará más tarde para confirmar\",\n",
      "  \"summary\": \"El cliente, Martín Pérez, llamó porque tenía problemas con su plan de datos móviles. El agente, Juan, explicó que el cliente había consumido casi todo su límite de datos, lo que había reducido su velocidad de navegación. El cliente se sintió engañado porque no sabía que algunas aplicaciones consumen datos en segundo plano. Juan ofreció cambiar el plan del cliente a uno superior, con más GB de datos y mayor velocidad, por un costo adicional. El cliente dijo que lo pensaría y llamaría más tarde para confirmar.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Registering the Chat Completion Semantic Function\n",
    "extract_fn_config = sk.SemanticFunctionConfig(prompt_config, prompt_template)\n",
    "extract_fn = kernel.register_semantic_function(\"CallCenterPlugin\", \"ChatExtract\", extract_fn_config)\n",
    "\n",
    "try:\n",
    "    # Create a context variable to hold the input and collecting the input from user\n",
    "    context_vars = sk.ContextVariables()\n",
    "    context_vars[\"input\"] = input\n",
    "\n",
    "    # Call the extract function with user input\n",
    "    extraction = await kernel.run_async(extract_fn, input_vars=context_vars)\n",
    "    print(f\"Extraction: {extraction}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")   \n",
    "    exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
