{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB02 - Introduction to Semantic Kernel\n",
    "\n",
    "In this lab, we will learn how to use Semantic Kernel to interact with Large Language Models. We'll complete the following steps:\n",
    "\n",
    "1) Import general dependencies.\n",
    "2) Using Semantic Kernel SDK to run prompts on a Text Completion model from Azure OpenAI. \n",
    "3) Instantiate a Kernel to add LLM capabilities to your application.\n",
    "4) Connect the Kernel to a Completion model to run your first semantic function.\n",
    "5) Run Chat Completion prompts using Semantic Kernel.\n",
    "\n",
    "**Important:** We're a using a method to automatically read sensitive parameters from a `.env` file. Therefore, please ensure that you have a `.env` file in the same directory of this notebook with the following parameters:\n",
    "\n",
    "``` python\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=\"NAME OF YOUR MODEL DEPLOYMENT\"\n",
    "AZURE_OPENAI_ENDPOINT=\"https://<YOUR_ENDPOINT>.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY=\"KEY\"\n",
    "```\n",
    "Optionally, you can update the values and run the cell below to create your `.env` file. Please note the deployment name as we'll use both **Text Completion** and **Chat Completion** in this LAB. Initially, we'll use a TEXT COMPLETION model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .env\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=\"NAME OF YOUR TEXT COMPLETION DEPLOYMENT\"\n",
    "AZURE_OPENAI_ENDPOINT=\"https://<YOUR_ENDPOINT>.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY=\"KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Import Dependencies\n",
    "\n",
    "**Semantic Kernel** is a SDK currently available in C#, Java and Python. However, each language support different at the current stage. Please visit [supported languages](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages) to check which features are supported per language.\n",
    "\n",
    "In this lab, we'll use Python SDK which can be obtained in Pypi repository.\n",
    "\n",
    "**Important:** if you get an error running the code below, make sure that you've completed the setup process. Optionally, you can run `pip install semantic_kernel` in your terminal or `!python -m pip install semantic_kernel` in a Python code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Semantic Kernel SDK that will be used in this lab. \n",
    "#The other dependencies will be imported directly in the step where they are used.\n",
    "import semantic_kernel as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Using Semantic Kernel SDK to run prompts on a Text Completion model from Azure OpenAI\n",
    "\n",
    "In this step, we'll connect to an **Azure OpenAI Service** and use a Text Completion model. \n",
    "The Semantic Kernel also supports **OpenAI**. Other AI Services, like **Hugging Face** will be supported shortly. You can visit this [link](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages#ai-service-endpoints) to check the most updated list.\n",
    "\n",
    "In this example, we're using **AzureTextCompletion** module to interact with Text Completion models from Azure OpenAI, such as `gpt-35-turbo-instruct` or `text-davinci-003`. This is suitable for text generation activities. We are also using a module to control the behavior of the requests, defining parameters like Temperature, max_tokens and even how many responses we want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Azure Text Completion service connector\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion\n",
    "#Importing the module that contains the settings for the Azure Text Completion service\n",
    "from semantic_kernel.connectors.ai import CompleteRequestSettings\n",
    "\n",
    "# Read the model, API key and endpoint from the .env file\n",
    "deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "\n",
    "# Create the Azure Text Completion service connector\n",
    "azure_text_completion = AzureTextCompletion(deployment, endpoint, api_key)\n",
    "\n",
    "# Create the settings to control the behavior to the requests of Azure Text Completion service\n",
    "request_settings = CompleteRequestSettings(\n",
    "    max_tokens=20,\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    number_of_responses=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all set, let's run a simple prompt to generate taglines using the Text Completion model. Please note that the number of results, response lenght and temperature are controlled by the **CompleteRequestSettings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write tagline for a telco company.\"\n",
    "\n",
    "#Calling the Azure Text Completion service using Semantic Kernel SDK and AI Service Connector\n",
    "results = await azure_text_completion.complete_async(prompt, request_settings)\n",
    "\n",
    "#Printing out the results\n",
    "i = 1\n",
    "for result in results:\n",
    "    print(f\"Result {i}: {result}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to use the SDK to run a prompt, let's start building out our Call Center Analytics Solution. We have already explored how to use prompts to summarize and extract relevant information from call center conversations. As our next step, let's run them in Semantic Kernel. We'll explore different methods.\n",
    "\n",
    "We'll use the conversation below as an example. If you prefer, just change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a call center conversation between a customer and an agent. You can replace by your own example\n",
    "input = \"\"\"\n",
    "Agente: Hola, bienvenido al servicio de atención al cliente de telco. Mi nombre es Juan, ¿en qué puedo ayudarlo?\n",
    "Cliente: Hola, Juan. Estoy llamando porque tengo problemas con mi plan de datos móviles. Es muy lento y no puedo navegar por internet ni usar mis aplicaciones.\n",
    "Agente: Lo siento mucho por las molestias, señor. ¿Me puede decir su número de teléfono y su nombre completo, por favor?\n",
    "Cliente: Sí, claro. Mi número es 011-4567-8910 y mi nombre es Martín Pérez.\n",
    "Agente: Gracias, señor Pérez. Voy a verificar su plan y su consumo de datos. Un momento, por favor.\n",
    "Cliente: Está bien, gracias.\n",
    "Agente: Señor Pérez, he revisado su plan y veo que tiene contratado el plan básico de 2 GB de datos por mes. ¿Es correcto?\n",
    "Cliente: Sí, así es.\n",
    "Agente: Bueno, le informo que ha consumido el 90% de su límite de datos y que le quedan solo 200 MB disponibles hasta el final del mes. Por eso su velocidad de navegación se ha reducido.\n",
    "Cliente: ¿Qué? ¿Cómo es posible? Yo apenas uso el internet en mi celular. Solo reviso mi correo y mis redes sociales de vez en cuando. No veo videos ni descargo archivos grandes.\n",
    "Agente: Entiendo, señor Pérez. Pero tenga en cuenta que algunas aplicaciones consumen datos en segundo plano, sin que usted se dé cuenta. Por ejemplo, las actualizaciones automáticas, las copias de seguridad, el GPS, etc.\n",
    "Cliente: Bueno, pero eso no me lo explicaron cuando contraté el plan. Me dijeron que con 2 GB tendría suficiente para todo el mes. Me siento engañado.\n",
    "Agente: Le pido disculpas, señor Pérez. No era nuestra intención engañarlo. Le ofrezco una solución: si quiere, puede cambiar su plan a uno superior, con más GB de datos y mayor velocidad. Así podrá disfrutar de una mejor experiencia de navegación.\n",
    "Cliente: ¿Y cuánto me costaría eso?\n",
    "Agente: Tenemos una oferta especial para usted. Por solo 10 pesos más al mes, puede acceder al plan premium de 5 GB de datos y velocidad 4G. ¿Le interesa?\n",
    "Cliente: Mmm, no sé. ¿No hay otra opción? ¿No pueden darme más velocidad sin cobrarme más?\n",
    "Agente: Lo siento, señor Pérez. Esa es la única opción que tenemos disponible. Si no cambia su plan, tendrá que esperar hasta el próximo mes para recuperar su velocidad normal. O puede comprar un paquete adicional de datos, pero le saldría más caro que cambiar de plan.\n",
    "Cliente: Bueno, déjeme pensarlo. ¿Puedo llamar más tarde para confirmar?\n",
    "Agente: Claro, señor Pérez. Puede llamar cuando quiera. El número es el mismo que marcó ahora. ¿Hay algo más en lo que pueda ayudarlo?\n",
    "Cliente: No, eso es todo. Gracias por su atención.\n",
    "Agente: Gracias a usted, señor Pérez. Que tenga un buen día. Adiós.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the same prompt for \"Problem Summarization\". Note that we're using python strings to append the Conversation to our prompt. As you run this cell, we should see a summary similar to what we saw in Azure AI Studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initially, let's use a prompt similar to what we have tested in the AI Studio.\n",
    "prompt = f\"\"\"\n",
    "Summarize the problem described below. \n",
    "Be brief, yet informative.\n",
    "Try to capture in the summary:\n",
    "- The problem description;\n",
    "- The product customer is complaining (if available);\n",
    "- How Agente, handled the call;\n",
    "- The outcome of the call. \n",
    "\n",
    "[INPUT]\n",
    "{input}\n",
    "[END INPUT]\n",
    "\"\"\"\n",
    "\n",
    "# Create the settings to control the behavior to the requests of Azure Text Completion service\n",
    "request_settings = CompleteRequestSettings(\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    number_of_responses=1\n",
    ")\n",
    "\n",
    "#Calling the Azure Text Completion service using Semantic Kernel SDK and AI Service Connector\n",
    "result = await azure_text_completion.complete_async(prompt, request_settings)\n",
    "\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Instantiate a Kernel to add LLM capabilities to your application\n",
    "The core of Semantic Kernel is it's Kernel. It's responsible to transform an User Input (or ASK) into a response, optionally orchestrating Plugins in this process. It's also what makes Semantic Kernel shines as it can transform existing applications in AI applications.\n",
    "\n",
    "In this step, we'll instantiate a simple Kernel with no additonal options. Optionally, we could define Logging, Memory and the set of Plugins we want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the kernel\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "\n",
    "#Example of how to instantiate the kernel with a different log level\n",
    "#logger = sk.NullLogger()\n",
    "#kernel_with_logging = sk.Kernel(log=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Connect the Kernel to a Completion model to run your first semantic function.\n",
    "Now that we have our kernel, we can start adding capabilities to it. We already saw how to use the SDK and connectors to the AI services to run prompts. Now, let's see how can we use the Kernel to transform a prompt into a semantic function able to run Text Completion activities using a LLM.\n",
    "\n",
    "The first step is to add to our kernel the Text Completion model that we already have defined before. We can add multiple models and give then names (or nicknames) to later decide which ones we want to use and define programmatically which one is the default.\n",
    "\n",
    "You can also add Chat Completion models, like GPT-4 and GPT-3.5-Turbo, as per the commented code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the text completion service to the kernel\n",
    "kernel.add_text_completion_service(\"Text Completion\", \n",
    "                                   AzureTextCompletion(deployment, endpoint, api_key))\n",
    "\n",
    "#We can add several models of the same type to the kernel\n",
    "#kernel.add_text_completion_service(\"Other Text Completion\",\n",
    "#                                   AzureTextCompletion(\"text-curie-001\", endpoint, api_key))\n",
    "\n",
    "#You can also define a default Text Completion / Chat Completion service to be used by the kernel \n",
    "#kernel.set_default_text_completion_service(\"Text Completion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the kernel connected to a large language model from Azure OpenAI Service, we can start using it to run Text Completion operations. The best way to make our prompts reusable is to create a **Semantic Function**. You can create and add one to the Kernel based on a **Prompt**. This is simple a textual description, or instruction, of what we want the function to do. Optionally, we can add parameters, like `{{$input}}`, to inject values in runtime.\n",
    "\n",
    "We'll deep dive in Semantic Functions later and now we'll demonstrate a simplified way to create and run Semantic Function **inline**. With this method we can start structuring our application and transforming Prompts into functions. Please note that as we define the function we can also control the behavior of how we expect the kernel to run it, for example, controlling the max number of tokens in the response and the temperature. This we'll ensure that the function we'll run consistenly.\n",
    "\n",
    "Therefore, in this step, we'll create an **inline semantic function** to classify a telco problem in different categories, like Mobile Internet, Fixed Internet, etc. This is based on the same example we run in Azure AI Studio and is one of the things we need to complete in our Call Center Analytics Solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the prompt for categorizing based on the problem description\n",
    "prompt = \"\"\"\n",
    "Problem Description: {{$input}}\n",
    "Classify the problem description above as one of these categories: Fixed Internet, Mobile Internet, TV, Landline, Billing.\n",
    "Only write the output category with no extra text.\n",
    "Only write one category per problem description.\n",
    "\"\"\"\n",
    "\n",
    "# Create a semantic function for classifying the problem description\n",
    "classify_call_function = kernel.create_semantic_function(prompt, \n",
    "                                                         max_tokens=10,\n",
    "                                                         temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our Kernel has a new function to Categorize problems, we'll test it based on a problem description. We expected as a result one of the classifications previously defined in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invoke the semantic function as a regular Python function\n",
    "try:\n",
    "    classification = classify_call_function(input)\n",
    "    print(f\"Classification: {classification}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also register the semantic function and already define important information to later reuse the function. The code below is an example and you can also use it to define semantic functions inline. This notation will be useful in the future as we create our AI Plugins, which we'll explore later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import PromptTemplate, PromptTemplateConfig, SemanticFunctionConfig\n",
    "\n",
    "#Define a configuration to control how the prompt will run in the model and define descriptions that can later be used by planners\n",
    "prompt_config = PromptTemplateConfig(\n",
    "    description=\"Classify a problem report in a call to the telco company based on provided categories.\",\n",
    "    type=\"completion\",\n",
    "    completion=PromptTemplateConfig.CompletionConfig(0.5, 0.0, 0.0, 0.0, 10),\n",
    "    input=PromptTemplateConfig.InputConfig(\n",
    "        parameters=[\n",
    "            PromptTemplateConfig.InputParameter(\n",
    "                name=\"input\", description=\"The problem reported by the user that needs to be classified. Required parameter.\", default_value=\"\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Transform the Prompt into a PromptTemplate object\n",
    "prompt_template = PromptTemplate(\n",
    "    template=prompt,\n",
    "    template_engine=kernel.prompt_template_engine,\n",
    "    prompt_config=prompt_config\n",
    ")\n",
    "\n",
    "#Defines the semantic function configuration set\n",
    "categorize_fn_config = SemanticFunctionConfig(prompt_config, prompt_template)\n",
    "\n",
    "#Finally, register the new semantic function as part of an AI plugin. Later we'll explore this notation in more details\n",
    "categorize_fn = kernel.register_semantic_function(\"CallCenterPlugin\",\"Categorize\", categorize_fn_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Call the classify function with a problem description\n",
    "    result = await kernel.run_async(categorize_fn, input_str=input)\n",
    "    # Print the result\n",
    "    print(f\"Result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 - Run Chat Completion prompts using Semantic Kernel\n",
    "\n",
    "Models like **GPT-4** and **GPT-35-Turbo** were trained to accept inputs formatted as a **Conversation**. They use a different type of API called **Chat Completion**. Therefore, we need to make some adaptations to run prompts in these models as they require to use a different notation with messages considering three roles:\n",
    "- **System:** used to to give instructions/context about to the model and how it should behave. Also known as System Message or Metaprompt.\n",
    "- **User:** used to represent messages from the user. This is the messages we'll try to respond. Also refered as User Message.\n",
    "- **Assistant:** used to represent messages/responses from the model. Also refered as Assistant Message.\n",
    "\n",
    "Now, let's see how we can run Chat Completions in Semantic Kernel.\n",
    "\n",
    "**Important:** As we'll now run **Chat Completion** operations, you need to review and run the cell below to recreate a `.env` file with a **Chat Completion** model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .env\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=\"NAME OF YOUR CHAT COMPLETION DEPLOYMENT\"\n",
    "AZURE_OPENAI_ENDPOINT=\"https://<YOUR_ENDPOINT>.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY=\"KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we need to import the correct connector for AzureChatCompletion and add a Chat Completion model to the kernel. After that, it's important to correctly setup our prompt adding the different roles (System, Assistant, User). Eventually, you can use this notation to add more messages to act as few-shot examples to obtain better results from the model.\n",
    "\n",
    "Please note that we're using as **System Message** the example we ran in Azure AI Studio for our Call Center Analytics solution to extract additional information from a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Semantic Kernel SDK that will be used in this lab. \n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "#Instantiating the kernel\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "# Read the model, API key and endpoint from the .env file\n",
    "deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "\n",
    "kernel.add_chat_service(\"Chat Completion\", \n",
    "                        AzureChatCompletion(deployment, endpoint, api_key))\n",
    "\n",
    "#Define a system message to model how the model should behave\n",
    "system_message = \"\"\"\n",
    "You're an AI assistant that helps telco company to extract valuable information from their conversations by creating JSON files for each conversation transcription you receive. You always try to extract and format as a JSON:\n",
    "1. Customer Name [name]\n",
    "2. Customer Contact Phone [phone]\n",
    "3. Main Topic of the Conversation [topic]\n",
    "4. Customer Sentiment (Neutral, Positive, Negative)[sentiment]\n",
    "5. How the Agent Handled the Conversation [agent_behavior]\n",
    "6. What was the FINAL Outcome of the Conversation [outcome]\n",
    "7. A really brief Summary of the Conversation [summary]\n",
    "\n",
    "Only extract information that you're sure. If you're unsure, write \"Unknown/Not Found\" in the JSON file.\n",
    "\"\"\"\n",
    "\n",
    "#Defining the chat Completion parameters, like max_tokens, temperature, top_p, etc.\n",
    "prompt_config = sk.PromptTemplateConfig.from_completion_parameters(\n",
    "    max_tokens=500, temperature=0.7, top_p=0.5\n",
    ")\n",
    "\n",
    "#Defining a prompt template setup the Three Roles in a Chat Completion Operation. Here we're defining input for the User Message\n",
    "prompt_template = sk.ChatPromptTemplate(\n",
    "    \"{{$input}}\", kernel.prompt_template_engine, prompt_config\n",
    ")\n",
    "\n",
    "#Ensure our prompt template have at least a System Message\n",
    "prompt_template.add_system_message(system_message)\n",
    "#Optionally, we can load user and assistant messages to use as Few Shot examples\n",
    "prompt_template.add_user_message(\"Hello, how are you?\")\n",
    "prompt_template.add_assistant_message(\"Hello! I can help you to extract valuable information from Call Center conversations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our prompt ready and configured, we'll register it as a new inline Semantic Function and invoke it as a function. We should see important information extracted from our conversation in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Registering the Chat Completion Semantic Function\n",
    "extract_fn_config = sk.SemanticFunctionConfig(prompt_config, prompt_template)\n",
    "extract_fn = kernel.register_semantic_function(\"CallCenterPlugin\", \"ChatExtract\", extract_fn_config)\n",
    "\n",
    "try:\n",
    "    # Create a context variable to hold the input and collecting the input from user\n",
    "    context_vars = sk.ContextVariables()\n",
    "    context_vars[\"input\"] = input(\"Enter a Conversation Transcription:> \")\n",
    "\n",
    "    # Call the extract function with user input\n",
    "    extraction = await kernel.run_async(extract_fn, input_vars=context_vars)\n",
    "    print(f\"Extraction: {extraction}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")   \n",
    "    exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
