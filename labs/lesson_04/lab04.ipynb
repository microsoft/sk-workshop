{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lab 04b: Semantic Advanced Topics\n",
    "#### Chat with Your Data Scenario Using Vector Search\n",
    "\n",
    "In this lab, we will explore how to create a virtual agent utilizing Semantic Kernel, Azure OpenAI models, and AI Search as a Vector Store. This agent will be capable of understanding and responding to user queries in a conversational manner.\n",
    "\n",
    "#### Resource Creation\n",
    "\n",
    "Before we begin, you will need to set up the following resources:\n",
    "- An Azure OpenAI service with a deployment of either GPT-4 or GPT-3.5-turbo through the Azure Portal. For optimal performance, we recommend using the GPT-4 model.\n",
    "- Within the same Azure OpenAI service, create a deployment for the text-embedding-ada-002 model (version 2) with the name `text-embedding-ada-002`.\n",
    "- Lastly, establish an Azure AI Search service to serve as our memory store.\n",
    "\n",
    "#### Kernel Configuration\n",
    "\n",
    "To connect the Semantic Kernel to these services, configure the environment variables in an **.env** file.\n",
    "Use the provided **.env.template** as a starting point, renaming it to **.env** and updating the variable values accordingly.\n",
    "\n",
    "#### Kernel Initialization\n",
    "\n",
    "Proceed to initialize the Semantic Kernel and register a persistent **Semantic Memory**. This memory will store and retrieve information as needed by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "from semantic_kernel.connectors.memory.azure_cognitive_search import AzureCognitiveSearchMemoryStore\n",
    "from semantic_kernel.core_skills import ConversationSummarySkill, TextMemorySkill\n",
    "import time\n",
    "\n",
    "# initalize and immport TextMemorySkill\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "\n",
    "deployment, api_key, endpoint, api_version = sk.azure_openai_settings_from_dot_env(include_api_version=True)\n",
    "kernel.add_chat_service(\"chat-completion\", AzureChatCompletion(deployment_name=deployment, endpoint=endpoint, api_key=api_key, api_version=api_version))\n",
    "kernel.add_text_embedding_generation_service(\"ada\",AzureTextEmbedding(\"text-embedding-ada-002\", endpoint=endpoint, api_key=api_key, api_version=api_version))\n",
    "\n",
    "# register AI Search as a memory store\n",
    "\n",
    "azure_ai_search_api_key, azure_ai_search_url = sk.azure_aisearch_settings_from_dot_env()\n",
    "kernel.register_memory_store(\n",
    "    memory_store=AzureCognitiveSearchMemoryStore(\n",
    "        vector_size=1536,\n",
    "        search_endpoint=azure_ai_search_url,\n",
    "        admin_key=azure_ai_search_api_key\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAGChat Plugin Creation\n",
    "\n",
    "Set up the RAGChat plugin, which will facilitate the retrieval of information from the memory store to answer user queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p plugins/RAGChat/Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting plugins/RAGChat/Chat/config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile plugins/RAGChat/Chat/config.json\n",
    "{\n",
    "     \"schema\": 1,\n",
    "     \"type\": \"completion\",\n",
    "     \"description\": \"Based on the user ask, conversation history, search the memory for sources and answer the user.\",\n",
    "     \"completion\": {\n",
    "          \"max_tokens\": 200,\n",
    "          \"temperature\": 0.8,\n",
    "          \"top_p\": 0.0,\n",
    "          \"presence_penalty\": 0.0,\n",
    "          \"frequency_penalty\": 0.0\n",
    "     },\n",
    "     \"input\": {\n",
    "          \"parameters\": [\n",
    "               {\n",
    "                    \"name\": \"ask\",\n",
    "                    \"description\": \"The user's ask.\",\n",
    "                    \"defaultValue\": \"\",\n",
    "                    \"required\": true\n",
    "               },\n",
    "               {\n",
    "                    \"name\": \"chat_history\",\n",
    "                    \"description\": \"The conversation history.\",\n",
    "                    \"defaultValue\": \"\",\n",
    "                    \"required\": true\n",
    "               }                \n",
    "          ]\n",
    "     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting plugins/RAGChat/Chat/skprompt.txt\n"
     ]
    }
   ],
   "source": [
    "## Task Goal\n",
    "The task goal is to generate an ANSWER based on the user QUESTION and the provided SOURCES.\n",
    " \n",
    "## Task instructions\n",
    " \n",
    "You will be given a list of SOURCES that you can use to ANSWER the QUESTION. \n",
    "You will be given a conversation HISTORY to give you more context. \n",
    "You must use the SOURCES to ANSWER the QUESTION. \n",
    "You must not use any other SOURCES.\n",
    "You must not use your own knowledge to ANSWER the QUESTION.\n",
    "Do not include the word \"ANSWER\" in your response.\n",
    "Always include the SOURCE name for each fact in the response, referencing it with square brackets, e.g., [info1.txt]. \n",
    "Do not combine SOURCES; list each source separately, e.g., [info1.txt][info2.pdf].\n",
    "\n",
    "## Task Input:\n",
    "\"QUESTION\": \"{{$ask}}\"\n",
    "\"HISTORY\": \"{{ConversationSummaryPlugin.SummarizeConversation $chat_history}}\"\n",
    "\"SOURCES\": \"{{TextMemoryPlugin.recall $ask}}\"\n",
    " \n",
    "## Task Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Store Population\n",
    "\n",
    "Load documents into the Semantic Memory. These documents will form the knowledge base that the virtual agent will use to provide informed responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents into memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading documents into memory\")\n",
    "\n",
    "await kernel.memory.save_information_async(\n",
    "    \"kb\", id=\"https://learn.microsoft.com/en-us/semantic-kernel/overview/\", text=\"[https://learn.microsoft.com/en-us/semantic-kernel/overview/] Semantic Kernel is an open-source SDK that lets you easily combine AI services like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C# and Python to build LLM AI models. By doing so, you can create AI apps that combine the best of both worlds.\"\n",
    ")\n",
    "await kernel.memory.save_information_async(\n",
    "    \"kb\", id=\"https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/\", text=\"[https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/] Effective prompt design is essential to achieving desired outcomes with LLM AI models. Prompt engineering, also known as prompt design, is an emerging field that requires creativity and attention to detail. It involves selecting the right words, phrases, symbols, and formats that guide the model in generating high-quality and relevant texts.\"\n",
    ")\n",
    "await kernel.memory.save_information_async(\n",
    "    \"kb\", id=\"https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/llm-models/\", text=\"[https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/llm-models/] A GPT model is a type of neural network that uses the transformer architecture to learn from large amounts of text data. The model has two main components: an encoder and a decoder. The encoder processes the input text and converts it into a sequence of vectors, called embeddings, that represent the meaning and context of each word. The decoder generates the output text by predicting the next word in the sequence, based on the embeddings and the previous words. The model uses a technique called attention to focus on the most relevant parts of the input and output texts, and to capture long-range dependencies and relationships between words. The model is trained by using a large corpus of texts as both the input and the output, and by minimizing the difference between the predicted and the actual words. The model can then be fine-tuned or adapted to specific tasks or domains, by using smaller and more specialized datasets.\"\n",
    ")\n",
    "await kernel.memory.save_information_async(\n",
    "    \"kb\", id=\"https://learn.microsoft.com/en-us/semantic-kernel/memories/\", text=\"[https://learn.microsoft.com/en-us/semantic-kernel/memories/] Embeddings are a way of representing words or other data as vectors in a high-dimensional space. Vectors are like arrows that have a direction and a length. High-dimensional means that the space has many dimensions, more than we can see or imagine. The idea is that similar words or data will have similar vectors, and different words or data will have different vectors. This helps us measure how related or unrelated they are, and also perform operations on them, such as adding, subtracting, multiplying, etc. Embeddings are useful for AI models because they can capture the meaning and context of words or data in a way that computers can understand and process.\"\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plugin Loading\n",
    "\n",
    "Import the necessary plugins and initialize the context for the virtual agent. These plugins will enable the agent to summarize conversations, recall text from memory, and engage in chat using the RAGChat mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plugins and initialize context\n",
    "\n",
    "text_memory_skill = kernel.import_skill(TextMemorySkill(), skill_name=\"TextMemoryPlugin\")\n",
    "conversation_summary_plugin = kernel.import_skill(ConversationSummarySkill(kernel=kernel), skill_name=\"ConversationSummaryPlugin\")\n",
    "ragchat_plugin = kernel.import_semantic_skill_from_directory(\"./plugins\", \"RAGChat\")\n",
    "\n",
    "context = kernel.create_new_context()\n",
    "context[sk.core_skills.TextMemorySkill.COLLECTION_PARAM] = \"kb\"\n",
    "context[sk.core_skills.TextMemorySkill.RELEVANCE_PARAM] = 0.8\n",
    "context[sk.core_skills.TextMemorySkill.LIMIT_PARAM] = 3\n",
    "context[\"chat_history\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Search Testing\n",
    "\n",
    "Conduct a test to ensure that the memory search functionality is working as intended. This will confirm that the agent can retrieve relevant information based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: [\"[https://learn.microsoft.com/en-us/semantic-kernel/overview/] Semantic Kernel is an open-source SDK that lets you easily combine AI services like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C# and Python to build LLM AI models. By doing so, you can create AI apps that combine the best of both worlds.\", \"[https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/] Effective prompt design is essential to achieving desired outcomes with LLM AI models. Prompt engineering, also known as prompt design, is an emerging field that requires creativity and attention to detail. It involves selecting the right words, phrases, symbols, and formats that guide the model in generating high-quality and relevant texts.\", \"[https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/llm-models/] A GPT model is a type of neural network that uses the transformer architecture to learn from large amounts of text data. The model has two main components: an encoder and a decoder. The encoder processes the input text and converts it into a sequence of vectors, called embeddings, that represent the meaning and context of each word. The decoder generates the output text by predicting the next word in the sequence, based on the embeddings and the previous words. The model uses a technique called attention to focus on the most relevant parts of the input and output texts, and to capture long-range dependencies and relationships between words. The model is trained by using a large corpus of texts as both the input and the output, and by minimizing the difference between the predicted and the actual words. The model can then be fine-tuned or adapted to specific tasks or domains, by using smaller and more specialized datasets.\"]\n",
      "Memory search: [https://learn.microsoft.com/en-us/semantic-kernel/overview/] Semantic Kernel is an open-source SDK that lets you easily combine AI services like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C# and Python to build LLM AI models. By doing so, you can create AI apps that combine the best of both worlds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask = \"What are LLM AI Apps?\"\n",
    "\n",
    "# first test the recall function.\n",
    "variables = sk.ContextVariables()\n",
    "variables[\"collection\"] = \"kb\"\n",
    "variables[\"relevance\"] = 0.8\n",
    "variables[\"limit\"] = 3\n",
    "variables[\"input\"] = ask\n",
    "output_context = await kernel.run_async(\n",
    "    text_memory_skill[\"recall\"],\n",
    "    input_vars = variables\n",
    ")\n",
    "if output_context.error_occurred:\n",
    "    print(output_context.last_error_description)\n",
    "else:\n",
    "    print(\"Recall:\", output_context.result)\n",
    "\n",
    "# memory search\n",
    "result = await kernel.memory.search_async(\"kb\", ask)\n",
    "print(f\"Memory search: {result[0].text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute Chat Loop\n",
    "\n",
    "Run the chat loop, allowing for a real-time interactive session with the virtual agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat(\n",
    "    kernel: sk.Kernel, chat_func: sk.SKFunctionBase, context: sk.SKContext\n",
    ") -> bool:\n",
    "    try:\n",
    "        user_input = input(\"User:> \")\n",
    "        context[\"ask\"] = user_input\n",
    "        print(f\"User:> {user_input}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    if user_input == \"exit\":\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    answer = await kernel.run_async(chat_func, input_vars=context.variables)\n",
    "    if answer.error_occurred:\n",
    "        answer = answer.last_error_description\n",
    "    \n",
    "    context[\"chat_history\"] += f\"\\nUser:> {user_input}\\nChatBot:> {answer}\\n\"\n",
    "\n",
    "    print(f\"ChatBot:> {answer}\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run chat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:> hello\n",
      "ChatBot:> Since there are no SOURCES provided and the QUESTION \"hello\" does not request specific information, a factual answer cannot be generated. If you have a specific question or need information, please provide more context or sources for reference.\n",
      "User:> what is semanti kernel?\n",
      "ChatBot:> Semantic Kernel is an open-source Software Development Kit (SDK) that enables the integration of AI services such as OpenAI, Azure OpenAI, and Hugging Face with traditional programming languages, including C# and Python. This integration facilitates the development of Large Language Model (LLM) AI applications that leverage the strengths of both AI services and conventional programming approaches [https://learn.microsoft.com/en-us/semantic-kernel/overview/].\n",
      "User:> and who is pele?\n",
      "ChatBot:> Since there are no SOURCES provided, I cannot give you a sourced answer about who Pelé is. If you can provide a source, I'd be happy to help with information from it.\n",
      "User:> ok\n",
      "ChatBot:> Since there are no SOURCES provided, I am unable to offer any factual information or answer your question. Please provide a source or more context so I can assist you effectively.\n",
      "User:> exit\n",
      "\n",
      "\n",
      "Exiting chat...\n"
     ]
    }
   ],
   "source": [
    "chatting = True\n",
    "while chatting:\n",
    "    chatting = await chat(kernel, ragchat_plugin[\"Chat\"], context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terminal Execution\n",
    "\n",
    "For execution outside of the Jupyter environment, use the terminal to run the chatbot script. This allows for a standalone operation of the virtual agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ChatBot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ChatBot.py\n",
    "\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "from semantic_kernel.connectors.memory.azure_cognitive_search import AzureCognitiveSearchMemoryStore\n",
    "from semantic_kernel.core_skills import ConversationSummarySkill, TextMemorySkill\n",
    "import asyncio\n",
    "\n",
    "# initalize and immport TextMemorySkill\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "\n",
    "deployment, api_key, endpoint, api_version = sk.azure_openai_settings_from_dot_env(include_api_version=True)\n",
    "kernel.add_chat_service(\"chat-completion\", AzureChatCompletion(deployment_name=deployment, endpoint=endpoint, api_key=api_key, api_version=api_version))\n",
    "kernel.add_text_embedding_generation_service(\"ada\",AzureTextEmbedding(\"text-embedding-ada-002\", endpoint=endpoint, api_key=api_key, api_version=api_version))\n",
    "\n",
    "# register AI Search as a memory store\n",
    "\n",
    "azure_ai_search_api_key, azure_ai_search_url = sk.azure_aisearch_settings_from_dot_env()\n",
    "kernel.register_memory_store(\n",
    "    memory_store=AzureCognitiveSearchMemoryStore(\n",
    "        vector_size=1536,\n",
    "        search_endpoint=azure_ai_search_url,\n",
    "        admin_key=azure_ai_search_api_key\n",
    "    )\n",
    ")\n",
    "\n",
    "# Import plugins and initialize context\n",
    "\n",
    "text_memory_skill = kernel.import_skill(TextMemorySkill(), skill_name=\"TextMemoryPlugin\")\n",
    "conversation_summary_plugin = kernel.import_skill(ConversationSummarySkill(kernel=kernel), skill_name=\"ConversationSummaryPlugin\")\n",
    "ragchat_plugin = kernel.import_semantic_skill_from_directory(\"./plugins\", \"RAGChat\")\n",
    "\n",
    "context = kernel.create_new_context()\n",
    "context[sk.core_skills.TextMemorySkill.COLLECTION_PARAM] = \"kb\"\n",
    "context[sk.core_skills.TextMemorySkill.RELEVANCE_PARAM] = 0.8\n",
    "context[sk.core_skills.TextMemorySkill.LIMIT_PARAM] = 3\n",
    "context[\"chat_history\"] = \"\"\n",
    "\n",
    "# Chat flow\n",
    "\n",
    "async def chat(\n",
    "    kernel: sk.Kernel, chat_func: sk.SKFunctionBase, context: sk.SKContext\n",
    ") -> bool:\n",
    "    try:\n",
    "        user_input = input(\"User:> \")\n",
    "        context[\"ask\"] = user_input\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    if user_input == \"exit\":\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    answer = await kernel.run_async(chat_func, input_vars=context.variables)\n",
    "    if answer.error_occurred:\n",
    "        answer = answer.last_error_description\n",
    "    \n",
    "    context[\"chat_history\"] += f\"\\nUser:> {user_input}\\nChatBot:> {answer}\\n\"\n",
    "\n",
    "    print(f\"ChatBot:> {answer}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "async def main():\n",
    "    chatting = True\n",
    "    while chatting:\n",
    "        chatting = await chat(kernel, ragchat_plugin[\"Chat\"], context)\n",
    "\n",
    "# Create an event loop\n",
    "loop = asyncio.get_event_loop()\n",
    "# Use the event loop to run the main function until it completes\n",
    "loop.run_until_complete(main())\n",
    "# Close the loop\n",
    "loop.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run this on your terminal:\n",
    "\n",
    "```\n",
    "cd lesson_04\n",
    "python src/ChatBot.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
